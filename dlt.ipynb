{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `dlt` Demo\n",
    "\n",
    "Resource: https://dlthub.com/docs/intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic\n",
    "\n",
    "`dlt` can be used in jupyter notebook or command line (config).\n",
    "\n",
    "You can create your own [transformer](https://dlthub.com/docs/dlt-ecosystem/verified-sources/filesystem/advanced#example-read-data-from-excel-files) to load excel files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "import duckdb\n",
    "\n",
    "# for data validation\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "from typing import Literal, Optional\n",
    "from decimal import Decimal\n",
    "\n",
    "# using read_csv_duckdb is much more efficient than read_csv, which uses pandas\n",
    "from dlt.sources.filesystem import filesystem, read_csv_duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BusinessRecord(BaseModel):\n",
    "    \"\"\"Represents a single business record in the dataset.\"\"\"\n",
    "\n",
    "    id: int = Field(gt=0, description=\"Unique identifier for the record\")\n",
    "    value: Decimal = Field(decimal_places=2, description=\"Business metric value\")\n",
    "    timestamp: datetime = Field(description=\"Timestamp of the record\")\n",
    "    description: str = Field(\n",
    "        min_length=1, description=\"Description of the business activity\"\n",
    "    )\n",
    "    category: Literal[\n",
    "        \"Finance\", \"Sales\", \"Customer Service\", \"Marketing\", \"HR\", \"IT\"\n",
    "    ] = Field(description=\"Business department category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch data from the [file system](https://dlthub.com/docs/dlt-ecosystem/verified-sources/filesystem/basic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesystem_resource_topic = filesystem(bucket_url=\"file:data/normal\", file_glob=\"*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add filters (filter by name or size) at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dlt.extract.resource.DltResource at 0x112e5bbf0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filesystem_resource_topic.add_filter(\n",
    "    lambda item: item[\"file_name\"]\n",
    "    not in [\"normal3.csv\", \"normal4_1.csv\", \"normal4_2.csv\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesystem_pipe_topic = filesystem_resource_topic | read_csv_duckdb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can apply hints (e.g., [incremental loading](https://dlthub.com/docs/general-usage/incremental-loading), i.e., [only load the new data](https://dlthub.com/docs/dlt-ecosystem/verified-sources/filesystem/basic#5-incremental-loading), create table name, and specify table schema) at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dlt.extract.resource.DltResource at 0x114856960>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filesystem_pipe_topic.apply_hints(\n",
    "    write_disposition=\"replace\", table_name=\"normal\", columns=BusinessRecord\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below generates a `example.duckdb` file. This file can be used in dbt via dbt-duckdb, see [this doc](https://dlthub.com/docs/dlt-ecosystem/destinations/duckdb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_topic = dlt.pipeline(\n",
    "    pipeline_name=\"csv_load\",\n",
    "    destination=dlt.destinations.duckdb(\"example.duckdb\"),\n",
    "    dataset_name=\"mydata\",\n",
    ")\n",
    "\n",
    "load_info = pipeline_topic.run(\n",
    "    filesystem_resource_topic\n",
    ")  # the hints can be passed here as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pipeline_topic.default_schema.to_pretty_yaml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = duckdb.connect(database=\"example.duckdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────┬───────────────────────┬─────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────┬───────────┐\n",
       "│ database │        schema         │        name         │                                               column_names                                               │                                       column_types                                       │ temporary │\n",
       "│ varchar  │        varchar        │       varchar       │                                                varchar[]                                                 │                                        varchar[]                                         │  boolean  │\n",
       "├──────────┼───────────────────────┼─────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────┼───────────┤\n",
       "│ example  │ mydata                │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                         │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                            │ false     │\n",
       "│ example  │ mydata                │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id]         │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]  │ false     │\n",
       "│ example  │ mydata                │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                                │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                    │ false     │\n",
       "│ example  │ mydata                │ filesystem          │ [file_name, relative_path, file_url, mime_type, modification_date, size_in_bytes, _dlt_load_id, _dlt_id] │ [VARCHAR, VARCHAR, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, BIGINT, VARCHAR, VARCHAR] │ false     │\n",
       "│ example  │ mydata                │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                                     │ [BIGINT, DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR]    │ false     │\n",
       "│ example  │ mydata_20250121080243 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                         │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                            │ false     │\n",
       "│ example  │ mydata_20250121080243 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id]         │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]  │ false     │\n",
       "│ example  │ mydata_20250121080243 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                                │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                    │ false     │\n",
       "│ example  │ mydata_20250121080243 │ join                │ [id, assigned_to, duration_minutes, status, cost_estimate, _dlt_load_id, _dlt_id]                        │ [BIGINT, VARCHAR, BIGINT, VARCHAR, DOUBLE, VARCHAR, VARCHAR]                             │ false     │\n",
       "│ example  │ mydata_20250121081457 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                         │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                            │ false     │\n",
       "│ example  │ mydata_20250121081457 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id]         │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]  │ false     │\n",
       "│ example  │ mydata_20250121081457 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                                │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                    │ false     │\n",
       "│ example  │ mydata_20250121081457 │ join                │ [id, assigned_to, duration_minutes, status, cost_estimate, _dlt_load_id, _dlt_id]                        │ [BIGINT, VARCHAR, BIGINT, VARCHAR, DOUBLE, VARCHAR, VARCHAR]                             │ false     │\n",
       "├──────────┴───────────────────────┴─────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────┴───────────┤\n",
       "│ 13 rows                                                                                                                                                                                                                                                        6 columns │\n",
       "└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.sql(\"DESCRIBE;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.sql(\"SELECT id, COUNT(*) FROM mydata.normal GROUP BY id;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use Pandas, the data can be accessed via [`ReadableDataset`](https://dlthub.com/docs/general-usage/dataset-access/dataset).\n",
    "\n",
    "In addition to transforming data using `duckdb` explicitly, you can use [the `dlt` SQL client](https://dlthub.com/docs/dlt-ecosystem/transformations/sql) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pipeline_topic.sql_client() as p:\n",
    "#     ans = p.execute_sql(\n",
    "#         \"SELECT category, COUNT(*) FROM mydata.normal GROUP BY category;\"\n",
    "#     )\n",
    "\n",
    "# ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesystem_resource_task = filesystem(\n",
    "    bucket_url=\"file:data/joinable\", file_glob=\"*.csv\"\n",
    ")\n",
    "\n",
    "filesystem_resource_task.add_filter(lambda item: item[\"file_name\"] != \"j03.csv\")\n",
    "\n",
    "filesystem_pipe_task = filesystem_resource_task | read_csv_duckdb()\n",
    "\n",
    "filesystem_pipe_task.apply_hints(write_disposition=\"append\", table_name=\"join\")\n",
    "\n",
    "pipeline_task = dlt.pipeline(\n",
    "    pipeline_name=\"csv_load_join\",\n",
    "    destination=dlt.destinations.duckdb(\"example.duckdb\"),\n",
    "    dataset_name=\"mydata\",\n",
    "    dev_mode=True,\n",
    ")\n",
    "\n",
    "load_info_task = pipeline_task.run(filesystem_pipe_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pipeline_task.sql_client() as p:\n",
    "#     ans = p.execute_sql(\n",
    "#         \"SELECT n.id, value, category, assigned_to, status FROM mydata.normal AS n JOIN mydata.join AS j ON n.id=j.id;\"\n",
    "#     )\n",
    "\n",
    "# print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use schema to specify data types and make sure the loaded data contains valid data only.\n",
    "\n",
    "In addition to creating a Pydantic schema, you can [adjust a schema](https://dlthub.com/docs/walkthroughs/adjust-a-schema) based on the autogenerated one.\n",
    "\n",
    "To be specific, [schema](https://dlthub.com/docs/general-usage/schema) \"describes the structure of normalized data (e.g., tables, columns, data types, etc.) and provides instructions on how the data should be processed and loaded.\" And normalized means that `dlt` will change the structure of the input data, such as feature names, data types, etc., to load it into the destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_topic_schema = dlt.pipeline(\n",
    "    import_schema_path=\"schemas/import\",  # path to the schema file (imported schema)\n",
    "    export_schema_path=\"schemas/export\",  # path to the schema file (exported schema)\n",
    "    pipeline_name=\"csv_load_schema\",\n",
    "    destination=dlt.destinations.duckdb(\"example_schema.duckdb\"),\n",
    "    dataset_name=\"mydata_schema\",\n",
    "    dev_mode=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadInfo(pipeline=<dlt.pipeline.pipeline.Pipeline object at 0x113fca330>, metrics={'1737450030.396387': [{'started_at': DateTime(2025, 1, 21, 9, 0, 30, 509694, tzinfo=Timezone('UTC')), 'finished_at': DateTime(2025, 1, 21, 9, 0, 30, 567931, tzinfo=Timezone('UTC')), 'job_metrics': {'_dlt_pipeline_state.f9707277ff.insert_values': LoadJobMetrics(job_id='_dlt_pipeline_state.f9707277ff.insert_values', file_path='/Users/alex/.dlt/pipelines/csv_load_schema/load/normalized/1737450030.396387/started_jobs/_dlt_pipeline_state.f9707277ff.0.insert_values', table_name='_dlt_pipeline_state', started_at=DateTime(2025, 1, 21, 9, 0, 30, 550258, tzinfo=Timezone('UTC')), finished_at=DateTime(2025, 1, 21, 9, 0, 30, 551075, tzinfo=Timezone('UTC')), state='completed', remote_url=None), 'normal.ab7724e601.insert_values': LoadJobMetrics(job_id='normal.ab7724e601.insert_values', file_path='/Users/alex/.dlt/pipelines/csv_load_schema/load/normalized/1737450030.396387/started_jobs/normal.ab7724e601.0.insert_values', table_name='normal', started_at=DateTime(2025, 1, 21, 9, 0, 30, 550211, tzinfo=Timezone('UTC')), finished_at=DateTime(2025, 1, 21, 9, 0, 30, 551664, tzinfo=Timezone('UTC')), state='completed', remote_url=None)}}]}, destination_type='dlt.destinations.duckdb', destination_displayable_credentials='duckdb:////Users/alex/CAPE/dlt-demo/example_schema.duckdb', destination_name='duckdb', environment=None, staging_type=None, staging_name=None, staging_displayable_credentials=None, destination_fingerprint='', dataset_name='mydata_schema_20250121090030', loads_ids=['1737450030.396387'], load_packages=[LoadPackageInfo(load_id='1737450030.396387', package_path='/Users/alex/.dlt/pipelines/csv_load_schema/load/loaded/1737450030.396387', state='loaded', schema=Schema csv_load_schema at 4681334416, schema_update={'_dlt_loads': {'columns': {'load_id': {'data_type': 'text', 'nullable': False, 'name': 'load_id'}, 'schema_name': {'data_type': 'text', 'nullable': True, 'name': 'schema_name'}, 'status': {'data_type': 'bigint', 'nullable': False, 'name': 'status'}, 'inserted_at': {'data_type': 'timestamp', 'nullable': False, 'name': 'inserted_at'}, 'schema_version_hash': {'data_type': 'text', 'nullable': True, 'name': 'schema_version_hash'}}, 'write_disposition': 'skip', 'resource': '_dlt_loads', 'description': 'Created by DLT. Tracks completed loads', 'name': '_dlt_loads'}, '_dlt_pipeline_state': {'columns': {'version': {'data_type': 'bigint', 'nullable': False, 'name': 'version'}, 'engine_version': {'data_type': 'bigint', 'nullable': False, 'name': 'engine_version'}, 'pipeline_name': {'data_type': 'text', 'nullable': False, 'name': 'pipeline_name'}, 'state': {'data_type': 'text', 'nullable': False, 'name': 'state'}, 'created_at': {'data_type': 'timestamp', 'nullable': False, 'name': 'created_at'}, 'version_hash': {'data_type': 'text', 'nullable': True, 'name': 'version_hash'}, '_dlt_load_id': {'data_type': 'text', 'nullable': False, 'name': '_dlt_load_id'}, '_dlt_id': {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True, 'row_key': True}}, 'write_disposition': 'append', 'file_format': 'preferred', 'resource': '_dlt_pipeline_state', 'name': '_dlt_pipeline_state', 'x-normalizer': {'seen-data': True}}, 'normal': {'columns': {'id': {'data_type': 'decimal', 'nullable': False, 'name': 'id'}, 'value': {'data_type': 'decimal', 'nullable': False, 'name': 'value'}, 'timestamp': {'data_type': 'timestamp', 'nullable': False, 'name': 'timestamp'}, 'description': {'data_type': 'text', 'nullable': False, 'name': 'description'}, 'category': {'data_type': 'text', 'nullable': False, 'name': 'category'}, '_dlt_load_id': {'name': '_dlt_load_id', 'data_type': 'text', 'nullable': False}, '_dlt_id': {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True, 'row_key': True}}, 'write_disposition': 'replace', 'schema_contract': {'tables': 'evolve', 'columns': 'discard_value', 'data_type': 'freeze'}, 'resource': '_read_csv_duckdb', 'name': 'normal', 'x-normalizer': {'seen-data': True}}, '_dlt_version': {'columns': {'version': {'data_type': 'bigint', 'nullable': False, 'name': 'version'}, 'engine_version': {'data_type': 'bigint', 'nullable': False, 'name': 'engine_version'}, 'inserted_at': {'data_type': 'timestamp', 'nullable': False, 'name': 'inserted_at'}, 'schema_name': {'data_type': 'text', 'nullable': False, 'name': 'schema_name'}, 'version_hash': {'data_type': 'text', 'nullable': False, 'name': 'version_hash'}, 'schema': {'data_type': 'text', 'nullable': False, 'name': 'schema'}}, 'write_disposition': 'skip', 'resource': '_dlt_version', 'description': 'Created by DLT. Tracks schema updates', 'name': '_dlt_version'}}, completed_at=DateTime(2025, 1, 21, 9, 0, 30, 565510, tzinfo=Timezone('UTC')), jobs={'failed_jobs': [], 'started_jobs': [], 'completed_jobs': [LoadJobInfo(state='completed_jobs', file_path='/Users/alex/.dlt/pipelines/csv_load_schema/load/loaded/1737450030.396387/completed_jobs/normal.ab7724e601.0.insert_values', file_size=1405, created_at=DateTime(2025, 1, 21, 9, 0, 30, 486062, tzinfo=Timezone('UTC')), elapsed=0.07944798469543457, job_file_info=ParsedLoadJobFileName(table_name='normal', file_id='ab7724e601', retry_count=0, file_format='insert_values'), failed_message=None), LoadJobInfo(state='completed_jobs', file_path='/Users/alex/.dlt/pipelines/csv_load_schema/load/loaded/1737450030.396387/completed_jobs/_dlt_pipeline_state.f9707277ff.0.insert_values', file_size=536, created_at=DateTime(2025, 1, 21, 9, 0, 30, 486226, tzinfo=Timezone('UTC')), elapsed=0.0792841911315918, job_file_info=ParsedLoadJobFileName(table_name='_dlt_pipeline_state', file_id='f9707277ff', retry_count=0, file_format='insert_values'), failed_message=None)], 'new_jobs': []})], first_run=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_topic_schema.run(filesystem_pipe_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌────────────────┬──────────────────────────────┬─────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────┬───────────┐\n",
       "│    database    │            schema            │        name         │                                           column_names                                           │                                         column_types                                         │ temporary │\n",
       "│    varchar     │           varchar            │       varchar       │                                            varchar[]                                             │                                          varchar[]                                           │  boolean  │\n",
       "├────────────────┼──────────────────────────────┼─────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────┼───────────┤\n",
       "│ example_schema │ mydata_schema_20250121080243 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                │ false     │\n",
       "│ example_schema │ mydata_schema_20250121080243 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]      │ false     │\n",
       "│ example_schema │ mydata_schema_20250121080243 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                        │ false     │\n",
       "│ example_schema │ mydata_schema_20250121080243 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [DECIMAL(38,9), DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│ example_schema │ mydata_schema_20250121081458 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                │ false     │\n",
       "│ example_schema │ mydata_schema_20250121081458 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]      │ false     │\n",
       "│ example_schema │ mydata_schema_20250121081458 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                        │ false     │\n",
       "│ example_schema │ mydata_schema_20250121081458 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [DECIMAL(38,9), DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│ example_schema │ mydata_schema_20250121090030 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                │ false     │\n",
       "│ example_schema │ mydata_schema_20250121090030 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]      │ false     │\n",
       "│ example_schema │ mydata_schema_20250121090030 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                        │ false     │\n",
       "│ example_schema │ mydata_schema_20250121090030 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [DECIMAL(38,9), DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "├────────────────┴──────────────────────────────┴─────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────┴───────────┤\n",
       "│ 12 rows                                                                                                                                                                                                                                                                 6 columns │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = duckdb.connect(database=\"example_schema.duckdb\")\n",
    "db.sql(\"DESCRIBE;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the folders and schemas are created, you can edit the schema in the `import` folder to take effect.\n",
    "\n",
    "Acceptable data types: `['text', 'double', 'bool', 'timestamp', 'bigint', 'binary', 'json', 'decimal', 'wei', 'date', 'time']`\n",
    "\n",
    "Note: You should keep the import schema as simple as possible and let dlt do the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadInfo(pipeline=<dlt.pipeline.pipeline.Pipeline object at 0x113fca330>, metrics={'1737450030.6707048': [{'started_at': DateTime(2025, 1, 21, 9, 0, 30, 746456, tzinfo=Timezone('UTC')), 'finished_at': DateTime(2025, 1, 21, 9, 0, 30, 774847, tzinfo=Timezone('UTC')), 'job_metrics': {'normal.89d5933c99.insert_values': LoadJobMetrics(job_id='normal.89d5933c99.insert_values', file_path='/Users/alex/.dlt/pipelines/csv_load_schema/load/normalized/1737450030.6707048/started_jobs/normal.89d5933c99.0.insert_values', table_name='normal', started_at=DateTime(2025, 1, 21, 9, 0, 30, 758502, tzinfo=Timezone('UTC')), finished_at=DateTime(2025, 1, 21, 9, 0, 30, 759823, tzinfo=Timezone('UTC')), state='completed', remote_url=None)}}]}, destination_type='dlt.destinations.duckdb', destination_displayable_credentials='duckdb:////Users/alex/CAPE/dlt-demo/example_schema.duckdb', destination_name='duckdb', environment=None, staging_type=None, staging_name=None, staging_displayable_credentials=None, destination_fingerprint='', dataset_name='mydata_schema_20250121090030', loads_ids=['1737450030.6707048'], load_packages=[LoadPackageInfo(load_id='1737450030.6707048', package_path='/Users/alex/.dlt/pipelines/csv_load_schema/load/loaded/1737450030.6707048', state='loaded', schema=Schema csv_load_schema at 4641687072, schema_update={}, completed_at=DateTime(2025, 1, 21, 9, 0, 30, 772414, tzinfo=Timezone('UTC')), jobs={'failed_jobs': [], 'started_jobs': [], 'completed_jobs': [LoadJobInfo(state='completed_jobs', file_path='/Users/alex/.dlt/pipelines/csv_load_schema/load/loaded/1737450030.6707048/completed_jobs/normal.89d5933c99.0.insert_values', file_size=1407, created_at=DateTime(2025, 1, 21, 9, 0, 30, 728187, tzinfo=Timezone('UTC')), elapsed=0.04422736167907715, job_file_info=ParsedLoadJobFileName(table_name='normal', file_id='89d5933c99', retry_count=0, file_format='insert_values'), failed_message=None)], 'new_jobs': []})], first_run=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_topic_schema.run(filesystem_pipe_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌────────────────┬──────────────────────────────┬─────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────┬───────────┐\n",
       "│    database    │            schema            │        name         │                                           column_names                                           │                                         column_types                                         │ temporary │\n",
       "│    varchar     │           varchar            │       varchar       │                                            varchar[]                                             │                                          varchar[]                                           │  boolean  │\n",
       "├────────────────┼──────────────────────────────┼─────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────┼───────────┤\n",
       "│ example_schema │ mydata_schema_20250121080243 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                │ false     │\n",
       "│ example_schema │ mydata_schema_20250121080243 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]      │ false     │\n",
       "│ example_schema │ mydata_schema_20250121080243 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                        │ false     │\n",
       "│ example_schema │ mydata_schema_20250121080243 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [DECIMAL(38,9), DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│ example_schema │ mydata_schema_20250121081458 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                │ false     │\n",
       "│ example_schema │ mydata_schema_20250121081458 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]      │ false     │\n",
       "│ example_schema │ mydata_schema_20250121081458 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                        │ false     │\n",
       "│ example_schema │ mydata_schema_20250121081458 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [DECIMAL(38,9), DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│ example_schema │ mydata_schema_20250121090030 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                │ false     │\n",
       "│ example_schema │ mydata_schema_20250121090030 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]      │ false     │\n",
       "│ example_schema │ mydata_schema_20250121090030 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                        │ false     │\n",
       "│ example_schema │ mydata_schema_20250121090030 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [DECIMAL(38,9), DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "├────────────────┴──────────────────────────────┴─────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────┴───────────┤\n",
       "│ 12 rows                                                                                                                                                                                                                                                                 6 columns │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = duckdb.connect(database=\"example_schema.duckdb\")\n",
    "db.sql(\"DESCRIBE;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contract\n",
    "\n",
    "[Contracts](https://dlthub.com/docs/general-usage/schema-contracts) define how a schema evolve with the future data. It is extremely useful for streaming/batch loading data and recieving data at a regular frequency.\n",
    "\n",
    "To build a contract, there are three levels: `tables`, `columns`, `data_type`, and four actions: `evolve`, `freeze`, `discard_row`, `discard_value`, to consider.\n",
    "\n",
    "|  | `evolve` | `freeze` | `discard_row` | `discard_value` |\n",
    "|---|---|---|---|---|\n",
    "| `tables` | Allow to add new tables | Error, not allow to add new tables | Only add metadata, all data is discarded | Only add metadata, all data is discarded |\n",
    "| `columns` | Allow to add new columns | Error, not allow to add new columns | The rows with new column(s) are discarded | The new column(s) are discarded |\n",
    "| `data_type` | Use [variant column](https://dlthub.com/docs/general-usage/schema#variant-columns) | Error, not allow to use variant column | The rows with different column(s) are discarded | The value with different column(s) are discarded |\n",
    "\n",
    "Note: Under `tables` scope, the table with same name is still acceptable. The constraint is only on the table with a different name.\n",
    "\n",
    "Note: For coercible data type, `dlt` coerces the data type implicitly regardless of contracts. The table above shows how `dlt` deals with non-coercible data types with contracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo for table and column contract\n",
    "filesystem_resource_task_con = filesystem(\n",
    "    bucket_url=\"file:data/normal\", file_glob=\"*.csv\"\n",
    ")\n",
    "\n",
    "filesystem_resource_task_con.add_filter(\n",
    "    lambda item: item[\"file_name\"]\n",
    "    not in [\"normal3.csv\", \"normal4_1.csv\", \"normal4_2.csv\"]\n",
    ")\n",
    "\n",
    "filesystem_pipe_task_con = filesystem_resource_task_con | read_csv_duckdb()\n",
    "\n",
    "filesystem_pipe_task_con.apply_hints(write_disposition=\"append\", table_name=\"normal\")\n",
    "\n",
    "pipeline_task_con = dlt.pipeline(\n",
    "    pipeline_name=\"csv_load_join_con\",\n",
    "    destination=dlt.destinations.duckdb(\"example_con.duckdb\"),\n",
    "    dataset_name=\"mydata\",\n",
    ")\n",
    "\n",
    "load_info_task_con = pipeline_task_con.run(\n",
    "    filesystem_pipe_task_con,\n",
    "    schema_contract={\"tables\": \"evolve\", \"columns\": \"evolve\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo for table contract\n",
    "# filesystem_resource_task_con2 = filesystem(\n",
    "#     bucket_url=\"file:data/normal\", file_glob=\"normal3.csv\"\n",
    "# )\n",
    "\n",
    "# filesystem_pipe_task_con2 = filesystem_resource_task_con2 | read_csv_duckdb()\n",
    "\n",
    "# filesystem_pipe_task_con2.apply_hints(write_disposition=\"append\", table_name=\"normal\")\n",
    "\n",
    "# load_info_task_con2 = pipeline_task_con.run(\n",
    "#     filesystem_pipe_task_con2, schema_contract={\"tables\": \"freeze\"}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo for column contract\n",
    "filesystem_resource_task_con2 = filesystem(\n",
    "    bucket_url=\"file:data/normal\", file_glob=\"normal3.csv\"\n",
    ")\n",
    "\n",
    "filesystem_pipe_task_con2 = filesystem_resource_task_con2 | read_csv_duckdb()\n",
    "\n",
    "filesystem_pipe_task_con2.apply_hints(write_disposition=\"append\", table_name=\"normal\")\n",
    "\n",
    "load_info_task_con2 = pipeline_task_con.run(\n",
    "    filesystem_pipe_task_con2,\n",
    "    schema_contract={\"tables\": \"evolve\", \"columns\": \"discard_value\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo for data type contract\n",
    "# class BusinessRecord2(BaseModel):\n",
    "#     \"\"\"Represents a single business record in the dataset.\"\"\"\n",
    "\n",
    "#     id: Optional[int] = Field(\n",
    "#         gt=0, description=\"Unique identifier for the record\"\n",
    "#     )  # to focus on data types, use Optional to allow NULL\n",
    "#     value: Decimal = Field(decimal_places=2, description=\"Business metric value\")\n",
    "#     timestamp: datetime = Field(description=\"Timestamp of the record\")\n",
    "#     description: str = Field(\n",
    "#         min_length=1, description=\"Description of the business activity\"\n",
    "#     )\n",
    "#     category: Literal[\n",
    "#         \"Finance\", \"Sales\", \"Customer Service\", \"Marketing\", \"HR\", \"IT\"\n",
    "#     ] = Field(description=\"Business department category\")\n",
    "\n",
    "\n",
    "# filesystem_resource_task_con = filesystem(\n",
    "#     bucket_url=\"file:data/normal\", file_glob=\"*.csv\"\n",
    "# )\n",
    "\n",
    "# filesystem_resource_task_con.add_filter(\n",
    "#     lambda item: item[\"file_name\"]\n",
    "#     not in [\"normal3.csv\", \"normal4_1.csv\", \"normal4_2.csv\"]\n",
    "# )\n",
    "\n",
    "# filesystem_pipe_task_con = filesystem_resource_task_con | read_csv_duckdb()\n",
    "\n",
    "# filesystem_pipe_task_con.apply_hints(\n",
    "#     write_disposition=\"append\", table_name=\"normal\", columns=BusinessRecord2\n",
    "# )\n",
    "\n",
    "# pipeline_task_con = dlt.pipeline(\n",
    "#     pipeline_name=\"csv_load_join_con\",\n",
    "#     destination=dlt.destinations.duckdb(\"example_con.duckdb\"),\n",
    "#     dataset_name=\"mydata\",\n",
    "# )\n",
    "\n",
    "# load_info_task_con = pipeline_task_con.run(\n",
    "#     filesystem_pipe_task_con,\n",
    "#     schema_contract={\"tables\": \"evolve\", \"columns\": \"evolve\", \"data_type\": \"evolve\"},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo for data type contract\n",
    "# filesystem_resource_task_con2 = filesystem(\n",
    "#     bucket_url=\"file:data/normal\", file_glob=\"normal4_2.csv\"\n",
    "# )\n",
    "\n",
    "# filesystem_pipe_task_con2 = filesystem_resource_task_con2 | read_csv_duckdb()\n",
    "\n",
    "# filesystem_pipe_task_con2.apply_hints(write_disposition=\"append\", table_name=\"normal\")\n",
    "\n",
    "# load_info_task_con2 = pipeline_task_con.run(\n",
    "#     filesystem_pipe_task_con2,\n",
    "#     schema_contract={\n",
    "#         \"tables\": \"evolve\",\n",
    "#         \"columns\": \"evolve\",\n",
    "#         \"data_type\": \"discard_value\",\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────┬─────────┬─────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────┬─────────────────────────────────────────────────────────────────────────────────────────┬───────────┐\n",
       "│  database   │ schema  │        name         │                                           column_names                                           │                                      column_types                                       │ temporary │\n",
       "│   varchar   │ varchar │       varchar       │                                            varchar[]                                             │                                        varchar[]                                        │  boolean  │\n",
       "├─────────────┼─────────┼─────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────┼───────────┤\n",
       "│ example_con │ mydata  │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                           │ false     │\n",
       "│ example_con │ mydata  │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│ example_con │ mydata  │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                   │ false     │\n",
       "│ example_con │ mydata  │ join                │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [BIGINT, DOUBLE, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR]          │ false     │\n",
       "│ example_con │ mydata  │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [BIGINT, DOUBLE, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR]          │ false     │\n",
       "└─────────────┴─────────┴─────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────┴─────────────────────────────────────────────────────────────────────────────────────────┴───────────┘"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = duckdb.connect(database=\"example_con.duckdb\")\n",
    "db.sql(\"DESCRIBE;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬────────┬──────────────────────────┬────────────────────────────────┬──────────────────┬───────────────────┬────────────────┐\n",
       "│  id   │ value  │        timestamp         │          description           │     category     │   _dlt_load_id    │    _dlt_id     │\n",
       "│ int64 │ double │ timestamp with time zone │            varchar             │     varchar      │      varchar      │    varchar     │\n",
       "├───────┼────────┼──────────────────────────┼────────────────────────────────┼──────────────────┼───────────────────┼────────────────┤\n",
       "│     1 │ 157.23 │ 2024-01-01 17:00:00+08   │ Annual financial report review │ Finance          │ 1737446563.690403 │ bjJ9yjJFUPxFHw │\n",
       "│     2 │ 293.45 │ 2024-01-01 18:30:00+08   │ Quarterly sales analysis       │ Sales            │ 1737446563.690403 │ EufkjJM7tK/mGQ │\n",
       "│     3 │ 432.18 │ 2024-01-01 19:45:00+08   │ Customer feedback summary      │ Customer Service │ 1737446563.690403 │ OzYPQ9L8htM5dg │\n",
       "│     4 │ 567.89 │ 2024-01-01 21:15:00+08   │ Product launch presentation    │ Marketing        │ 1737446563.690403 │ u1gUxSgOIlzhsA │\n",
       "│     5 │ 123.45 │ 2024-01-01 22:30:00+08   │ Team performance evaluation    │ HR               │ 1737446563.690403 │ WCbFM1xlB1PckA │\n",
       "│     6 │ 789.12 │ 2024-01-01 23:45:00+08   │ Market research findings       │ Marketing        │ 1737446563.690403 │ 4o1aumetpM53oA │\n",
       "│     7 │ 234.56 │ 2024-01-02 00:00:00+08   │ Budget planning meeting        │ Finance          │ 1737446563.690403 │ bRWZUA24JrGOMg │\n",
       "│     8 │  678.9 │ 2024-01-02 17:30:00+08   │ Sales team training            │ Sales            │ 1737446563.690403 │ vXVHq0OXRnpdCg │\n",
       "│     9 │ 345.67 │ 2024-01-02 18:45:00+08   │ Customer survey results        │ Customer Service │ 1737446563.690403 │ Mj9dzrmBgT67ow │\n",
       "│    10 │ 891.23 │ 2024-01-02 19:30:00+08   │ Social media campaign          │ Marketing        │ 1737446563.690403 │ Yjusxa3uuixVlg │\n",
       "│     · │    ·   │           ·              │          ·                     │    ·             │         ·         │       ·        │\n",
       "│     · │    ·   │           ·              │          ·                     │    ·             │         ·         │       ·        │\n",
       "│     · │    ·   │           ·              │          ·                     │    ·             │         ·         │       ·        │\n",
       "│    36 │ 198.76 │ 2024-01-05 23:45:00+08   │ Annual budget review           │ Finance          │ 1737446563.828956 │ E6tZIz3YzQFnqA │\n",
       "│    37 │ 654.32 │ 2024-01-06 00:30:00+08   │ E-commerce platform upgrade    │ IT               │ 1737446563.828956 │ SeFTEwu7h5ccXw │\n",
       "│    38 │ 987.65 │ 2024-01-06 17:15:00+08   │ Seasonal sales promotion       │ Sales            │ 1737446563.828956 │ aSiqf2v6GyNnNw │\n",
       "│    39 │ 321.98 │ 2024-01-06 18:30:00+08   │ Support process optimization   │ Customer Service │ 1737446563.828956 │ IZSqMXuvp8XXBA │\n",
       "│    40 │ 765.43 │ 2024-01-06 19:45:00+08   │ Social media analytics         │ Marketing        │ 1737446563.828956 │ 3le7RWCokr2bwQ │\n",
       "│    41 │  432.1 │ 2024-01-06 21:00:00+08   │ Remote work policy update      │ HR               │ 1737446563.828956 │ SL1meGs9Mga4iA │\n",
       "│    42 │ 876.54 │ 2024-01-06 22:15:00+08   │ Financial risk assessment      │ Finance          │ 1737446563.828956 │ /Evh7TQ0jM8K+A │\n",
       "│    43 │ 543.21 │ 2024-01-06 23:30:00+08   │ Cloud migration project        │ IT               │ 1737446563.828956 │ Nr7S4SD8/YfGpg │\n",
       "│    44 │ 198.76 │ 2024-01-07 00:45:00+08   │ Customer win-back campaign     │ Sales            │ 1737446563.828956 │ EfiEI487tFeOxA │\n",
       "│    45 │ 654.32 │ 2024-01-07 01:00:00+08   │ Service level agreement review │ Customer Service │ 1737446563.828956 │ CXZ23css7gDCeA │\n",
       "├───────┴────────┴──────────────────────────┴────────────────────────────────┴──────────────────┴───────────────────┴────────────────┤\n",
       "│ 45 rows (20 shown)                                                                                                       7 columns │\n",
       "└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.sql(\"SELECT * FROM mydata.join;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Loading\n",
    "\n",
    "[Incremental loading](https://dlthub.com/docs/general-usage/incremental-loading) is useful for stateful data to track the most recent data. For example, refreshing individual's cash amount in a bank account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo for append and replace loading\n",
    "# filesystem_resource_task_inc = filesystem(\n",
    "#     bucket_url=\"file:data/inc\", file_glob=\"inc_1.csv\"\n",
    "# )\n",
    "\n",
    "# filesystem_resource_task_inc2 = filesystem(\n",
    "#     bucket_url=\"file:data/inc\", file_glob=\"inc_2.csv\"\n",
    "# )\n",
    "\n",
    "# filesystem_pipe_task_inc = filesystem_resource_task_inc | read_csv_duckdb()\n",
    "\n",
    "# filesystem_pipe_task_inc.apply_hints(write_disposition=\"append\", table_name=\"inc\")\n",
    "\n",
    "# pipeline_task_inc = dlt.pipeline(\n",
    "#     pipeline_name=\"csv_load_inc\",\n",
    "#     destination=dlt.destinations.duckdb(\"example_inc.duckdb\"),\n",
    "#     dataset_name=\"mydata\",\n",
    "# )\n",
    "\n",
    "# load_info_task_inc = pipeline_task_inc.run(filesystem_pipe_task_inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo for append loading\n",
    "# filesystem_pipe_task_inc2 = filesystem_resource_task_inc2 | read_csv_duckdb()\n",
    "\n",
    "# filesystem_pipe_task_inc2.apply_hints(write_disposition=\"append\", table_name=\"inc\")\n",
    "\n",
    "# load_info_task_inc2 = pipeline_task_inc.run(filesystem_pipe_task_inc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo for replace loading\n",
    "# filesystem_pipe_task_inc2 = filesystem_resource_task_inc2 | read_csv_duckdb()\n",
    "\n",
    "# filesystem_pipe_task_inc2.apply_hints(write_disposition=\"replace\", table_name=\"inc\")\n",
    "\n",
    "# load_info_task_inc2 = pipeline_task_inc.run(filesystem_pipe_task_inc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo for merge loading\n",
    "# filesystem_resource_task_inc = filesystem(\n",
    "#     bucket_url=\"file:data/inc\", file_glob=\"inc_1.csv\"\n",
    "# )\n",
    "\n",
    "# filesystem_pipe_task_inc = filesystem_resource_task_inc | read_csv_duckdb()\n",
    "\n",
    "# update the data based on timestamp (latest timestamp will be kept) and primary key\n",
    "# filesystem_pipe_task_inc.apply_hints(\n",
    "#     write_disposition=\"merge\",\n",
    "#     primary_key=\"id\",\n",
    "#     columns={\"timestamp\": {\"dedup_sort\": \"desc\"}},\n",
    "#     table_name=\"inc\",\n",
    "# )\n",
    "\n",
    "# pipeline_task_inc = dlt.pipeline(\n",
    "#     pipeline_name=\"csv_load_inc\",\n",
    "#     destination=dlt.destinations.duckdb(\"example_inc.duckdb\"),\n",
    "#     dataset_name=\"mydata\",\n",
    "# )\n",
    "\n",
    "# load_info_task_inc = pipeline_task_inc.run(filesystem_pipe_task_inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo for merge loading (scd2)\n",
    "filesystem_resource_task_inc = filesystem(\n",
    "    bucket_url=\"file:data/inc\", file_glob=\"inc_1_scd2.csv\"\n",
    ")\n",
    "\n",
    "filesystem_resource_task_inc2 = filesystem(\n",
    "    bucket_url=\"file:data/inc\", file_glob=\"inc_2_scd2.csv\"\n",
    ")\n",
    "\n",
    "filesystem_pipe_task_inc = filesystem_resource_task_inc | read_csv_duckdb()\n",
    "\n",
    "filesystem_pipe_task_inc.apply_hints(\n",
    "    write_disposition={\"disposition\": \"merge\", \"strategy\": \"scd2\"},\n",
    "    primary_key=\"id\",\n",
    "    table_name=\"inc\",\n",
    ")\n",
    "\n",
    "pipeline_task_inc = dlt.pipeline(\n",
    "    pipeline_name=\"csv_load_inc\",\n",
    "    destination=dlt.destinations.duckdb(\"example_inc.duckdb\"),\n",
    "    dataset_name=\"mydata\",\n",
    ")\n",
    "\n",
    "load_info_task_inc = pipeline_task_inc.run(filesystem_pipe_task_inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo for merge loading (scd2)\n",
    "filesystem_pipe_task_inc2 = filesystem_resource_task_inc2 | read_csv_duckdb()\n",
    "\n",
    "filesystem_pipe_task_inc2.apply_hints(\n",
    "    write_disposition={\"disposition\": \"merge\", \"strategy\": \"scd2\"},\n",
    "    primary_key=\"id\",\n",
    "    table_name=\"inc\",\n",
    ")\n",
    "\n",
    "load_info_task_inc2 = pipeline_task_inc.run(filesystem_pipe_task_inc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────┬────────────────┬─────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬───────────┐\n",
       "│  database   │     schema     │        name         │                                           column_names                                           │                                                   column_types                                                   │ temporary │\n",
       "│   varchar   │    varchar     │       varchar       │                                            varchar[]                                             │                                                    varchar[]                                                     │  boolean  │\n",
       "├─────────────┼────────────────┼─────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼───────────┤\n",
       "│ example_inc │ mydata         │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                                    │ false     │\n",
       "│ example_inc │ mydata         │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                          │ false     │\n",
       "│ example_inc │ mydata         │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                                            │ false     │\n",
       "│ example_inc │ mydata         │ inc                 │ [_dlt_valid_from, _dlt_valid_to, id, value, timestamp, _dlt_load_id, _dlt_id]                    │ [TIMESTAMP WITH TIME ZONE, TIMESTAMP WITH TIME ZONE, BIGINT, DOUBLE, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR] │ false     │\n",
       "│ example_inc │ mydata_staging │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                                            │ false     │\n",
       "│ example_inc │ mydata_staging │ inc                 │ [_dlt_valid_from, _dlt_valid_to, id, value, timestamp, _dlt_load_id, _dlt_id]                    │ [TIMESTAMP WITH TIME ZONE, TIMESTAMP WITH TIME ZONE, BIGINT, DOUBLE, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR] │ false     │\n",
       "└─────────────┴────────────────┴─────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴───────────┘"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = duckdb.connect(database=\"example_inc.duckdb\")\n",
    "db.sql(\"DESCRIBE;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────────────────────────────┬───────────────────────────────┬───────┬────────┬──────────────────────────┬────────────────────┬────────────────┐\n",
       "│        _dlt_valid_from        │         _dlt_valid_to         │  id   │ value  │        timestamp         │    _dlt_load_id    │    _dlt_id     │\n",
       "│   timestamp with time zone    │   timestamp with time zone    │ int64 │ double │ timestamp with time zone │      varchar       │    varchar     │\n",
       "├───────────────────────────────┼───────────────────────────────┼───────┼────────┼──────────────────────────┼────────────────────┼────────────────┤\n",
       "│ 2025-01-21 16:28:03.644273+08 │ 2025-01-21 16:28:42.375992+08 │     1 │ 756.23 │ 2024-01-21 17:00:00+08   │ 1737448083.6442728 │ gvW0po4Bs+4Fdw │\n",
       "│ 2025-01-21 16:28:03.644273+08 │ 2025-01-21 16:28:42.375992+08 │     2 │ 432.89 │ 2024-01-21 17:15:00+08   │ 1737448083.6442728 │ 1Uk/VovFwdekhA │\n",
       "│ 2025-01-21 16:28:03.644273+08 │ 2025-01-21 16:28:42.375992+08 │     3 │ 234.67 │ 2024-01-21 17:45:00+08   │ 1737448083.6442728 │ CvahGlMCVe8uTA │\n",
       "│ 2025-01-21 16:28:03.644273+08 │ 2025-01-21 16:28:42.375992+08 │     4 │ 456.78 │ 2024-01-21 19:00:00+08   │ 1737448083.6442728 │ eTHC1l5SWLw4iA │\n",
       "│ 2025-01-21 16:28:42.375992+08 │ 2025-01-21 17:00:31.270392+08 │     1 │ 789.23 │ 2024-01-21 19:15:00+08   │ 1737448122.375992  │ 6hjQS9Nx5mMVuQ │\n",
       "│ 2025-01-21 16:28:42.375992+08 │ 2025-01-21 17:00:31.270392+08 │     3 │ 234.56 │ 2024-01-21 19:30:00+08   │ 1737448122.375992  │ Mz/H6NKorrxUCw │\n",
       "│ 2025-01-21 16:28:42.375992+08 │ 2025-01-21 17:00:31.270392+08 │     4 │  678.9 │ 2024-01-21 19:45:00+08   │ 1737448122.375992  │ wml2AHG7p7xukw │\n",
       "│ 2025-01-21 16:28:42.375992+08 │ 2025-01-21 17:00:31.270392+08 │     2 │ 345.67 │ 2024-01-21 20:00:00+08   │ 1737448122.375992  │ n5SFXK324VKNCA │\n",
       "│ 2025-01-21 17:00:31.270392+08 │ 2025-01-21 17:00:31.385502+08 │     1 │ 756.23 │ 2024-01-21 17:00:00+08   │ 1737450031.2703922 │ gvW0po4Bs+4Fdw │\n",
       "│ 2025-01-21 17:00:31.270392+08 │ 2025-01-21 17:00:31.385502+08 │     2 │ 432.89 │ 2024-01-21 17:15:00+08   │ 1737450031.2703922 │ 1Uk/VovFwdekhA │\n",
       "│ 2025-01-21 17:00:31.270392+08 │ 2025-01-21 17:00:31.385502+08 │     3 │ 234.67 │ 2024-01-21 17:45:00+08   │ 1737450031.2703922 │ CvahGlMCVe8uTA │\n",
       "│ 2025-01-21 17:00:31.270392+08 │ 2025-01-21 17:00:31.385502+08 │     4 │ 456.78 │ 2024-01-21 19:00:00+08   │ 1737450031.2703922 │ eTHC1l5SWLw4iA │\n",
       "│ 2025-01-21 17:00:31.385502+08 │ NULL                          │     1 │ 789.23 │ 2024-01-21 19:15:00+08   │ 1737450031.3855019 │ 6hjQS9Nx5mMVuQ │\n",
       "│ 2025-01-21 17:00:31.385502+08 │ NULL                          │     3 │ 234.56 │ 2024-01-21 19:30:00+08   │ 1737450031.3855019 │ Mz/H6NKorrxUCw │\n",
       "│ 2025-01-21 17:00:31.385502+08 │ NULL                          │     4 │  678.9 │ 2024-01-21 19:45:00+08   │ 1737450031.3855019 │ wml2AHG7p7xukw │\n",
       "│ 2025-01-21 17:00:31.385502+08 │ NULL                          │     2 │ 345.67 │ 2024-01-21 20:00:00+08   │ 1737450031.3855019 │ n5SFXK324VKNCA │\n",
       "├───────────────────────────────┴───────────────────────────────┴───────┴────────┴──────────────────────────┴────────────────────┴────────────────┤\n",
       "│ 16 rows                                                                                                                               7 columns │\n",
       "└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.sql(\"SELECT * FROM mydata.inc;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
