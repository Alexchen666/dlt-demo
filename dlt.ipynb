{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `dlt` Demo\n",
    "\n",
    "Test data: 3 columns with different data types (they may have wrong types)\n",
    "\n",
    "1. How does dlt deal with each of them?\n",
    "2. How to filter data (based on time, e.g., only new data)\n",
    "3. How does dlt evolve with new data?\n",
    "4. How does the data type change? (e.g., input json output csv?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic\n",
    "\n",
    "`dlt` can be used in jupyter notebook or command line (config).\n",
    "\n",
    "You can create your own [transformer](https://dlthub.com/docs/dlt-ecosystem/verified-sources/filesystem/advanced#example-read-data-from-excel-files) to load excel files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "import duckdb\n",
    "\n",
    "# for data validation\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "from typing import List, Literal\n",
    "from decimal import Decimal\n",
    "\n",
    "# using read_csv_duckdb is much more efficient than read_csv, which uses pandas\n",
    "from dlt.sources.filesystem import filesystem, read_csv_duckdb, read_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BusinessRecord(BaseModel):\n",
    "    \"\"\"Represents a single business record in the dataset.\"\"\"\n",
    "    id: int = Field(gt=0, description=\"Unique identifier for the record\")\n",
    "    value: Decimal = Field(decimal_places=2, description=\"Business metric value\")\n",
    "    timestamp: datetime = Field(description=\"Timestamp of the record\")\n",
    "    description: str = Field(min_length=1, description=\"Description of the business activity\")\n",
    "    category: Literal[\"Finance\", \"Sales\", \"Customer Service\", \"Marketing\", \"HR\", \"IT\"] = Field(\n",
    "        description=\"Business department category\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesystem_resource_topic = filesystem(\n",
    "    bucket_url='file:data/normal',\n",
    "    file_glob='*.csv'\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add filters (filter by name or size) at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dlt.extract.resource.DltResource at 0x106d87bc0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filesystem_resource_topic.add_filter(lambda item: item['file_name'] != 'normal3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesystem_pipe_topic = filesystem_resource_topic | read_csv_duckdb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can apply hints (e.g., [incremental loading](https://dlthub.com/docs/general-usage/incremental-loading), i.e., only load the new data, create table name, and specify table schema) at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dlt.extract.resource.DltResource at 0x107de4c20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filesystem_pipe_topic.apply_hints(write_disposition='replace', table_name='normal', columns=BusinessRecord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below generates a `example.duckdb` file. This file can be used in dbt via dbt-duckdb, see [this doc](https://dlthub.com/docs/dlt-ecosystem/destinations/duckdb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_topic = dlt.pipeline(\n",
    "    pipeline_name='csv_load', \n",
    "    destination=dlt.destinations.duckdb('example.duckdb'), \n",
    "    dataset_name='mydata', dev_mode=True\n",
    ")\n",
    "\n",
    "load_info = pipeline_topic.run(filesystem_pipe_topic) # the hints can be passed here as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version: 2\n",
      "version_hash: kIJkEwsqD4hyhNTH5Foe2+2EZdk/ww6glM0dcOkA+1E=\n",
      "engine_version: 11\n",
      "name: csv_load\n",
      "tables:\n",
      "  _dlt_version:\n",
      "    columns:\n",
      "      version:\n",
      "        data_type: bigint\n",
      "        nullable: false\n",
      "      engine_version:\n",
      "        data_type: bigint\n",
      "        nullable: false\n",
      "      inserted_at:\n",
      "        data_type: timestamp\n",
      "        nullable: false\n",
      "      schema_name:\n",
      "        data_type: text\n",
      "        nullable: false\n",
      "      version_hash:\n",
      "        data_type: text\n",
      "        nullable: false\n",
      "      schema:\n",
      "        data_type: text\n",
      "        nullable: false\n",
      "    write_disposition: skip\n",
      "    resource: _dlt_version\n",
      "    description: Created by DLT. Tracks schema updates\n",
      "  _dlt_loads:\n",
      "    columns:\n",
      "      load_id:\n",
      "        data_type: text\n",
      "        nullable: false\n",
      "      schema_name:\n",
      "        data_type: text\n",
      "        nullable: true\n",
      "      status:\n",
      "        data_type: bigint\n",
      "        nullable: false\n",
      "      inserted_at:\n",
      "        data_type: timestamp\n",
      "        nullable: false\n",
      "      schema_version_hash:\n",
      "        data_type: text\n",
      "        nullable: true\n",
      "    write_disposition: skip\n",
      "    resource: _dlt_loads\n",
      "    description: Created by DLT. Tracks completed loads\n",
      "  normal:\n",
      "    columns:\n",
      "      id:\n",
      "        data_type: bigint\n",
      "        nullable: false\n",
      "      value:\n",
      "        data_type: decimal\n",
      "        nullable: false\n",
      "      timestamp:\n",
      "        data_type: timestamp\n",
      "        nullable: false\n",
      "      description:\n",
      "        data_type: text\n",
      "        nullable: false\n",
      "      category:\n",
      "        data_type: text\n",
      "        nullable: false\n",
      "      _dlt_load_id:\n",
      "        data_type: text\n",
      "        nullable: false\n",
      "      _dlt_id:\n",
      "        data_type: text\n",
      "        nullable: false\n",
      "        unique: true\n",
      "        row_key: true\n",
      "    write_disposition: replace\n",
      "    schema_contract:\n",
      "      tables: evolve\n",
      "      columns: discard_value\n",
      "      data_type: freeze\n",
      "    resource: _read_csv_duckdb\n",
      "    x-normalizer:\n",
      "      seen-data: true\n",
      "  _dlt_pipeline_state:\n",
      "    columns:\n",
      "      version:\n",
      "        data_type: bigint\n",
      "        nullable: false\n",
      "      engine_version:\n",
      "        data_type: bigint\n",
      "        nullable: false\n",
      "      pipeline_name:\n",
      "        data_type: text\n",
      "        nullable: false\n",
      "      state:\n",
      "        data_type: text\n",
      "        nullable: false\n",
      "      created_at:\n",
      "        data_type: timestamp\n",
      "        nullable: false\n",
      "      version_hash:\n",
      "        data_type: text\n",
      "        nullable: true\n",
      "      _dlt_load_id:\n",
      "        data_type: text\n",
      "        nullable: false\n",
      "      _dlt_id:\n",
      "        data_type: text\n",
      "        nullable: false\n",
      "        unique: true\n",
      "        row_key: true\n",
      "    write_disposition: append\n",
      "    file_format: preferred\n",
      "    resource: _dlt_pipeline_state\n",
      "    x-normalizer:\n",
      "      seen-data: true\n",
      "settings:\n",
      "  detections:\n",
      "  - iso_timestamp\n",
      "  default_hints:\n",
      "    not_null:\n",
      "    - _dlt_id\n",
      "    - _dlt_root_id\n",
      "    - _dlt_parent_id\n",
      "    - _dlt_list_idx\n",
      "    - _dlt_load_id\n",
      "    parent_key:\n",
      "    - _dlt_parent_id\n",
      "    root_key:\n",
      "    - _dlt_root_id\n",
      "    unique:\n",
      "    - _dlt_id\n",
      "    row_key:\n",
      "    - _dlt_id\n",
      "normalizers:\n",
      "  names: snake_case\n",
      "  json:\n",
      "    module: dlt.common.normalizers.json.relational\n",
      "previous_hashes:\n",
      "- /222yXpzZcE3aEb/cdmENA4A54pC2FdAtJc9G+vsIVg=\n",
      "- j52+Yv1oDL4uwZ07RAFnuEhR9GdpV74SB+h60Gy7zFI=\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_topic.default_schema.to_pretty_yaml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = duckdb.connect(database='example.duckdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────┬───────────────────────┬─────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────┬───────────┐\n",
       "│ database │        schema         │        name         │                                           column_names                                           │                                         column_types                                         │ temporary │\n",
       "│ varchar  │        varchar        │       varchar       │                                            varchar[]                                             │                                          varchar[]                                           │  boolean  │\n",
       "├──────────┼───────────────────────┼─────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────┼───────────┤\n",
       "│ example  │ mydata_20250116095815 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                │ false     │\n",
       "│ example  │ mydata_20250116095815 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]      │ false     │\n",
       "│ example  │ mydata_20250116095815 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                        │ false     │\n",
       "│ example  │ mydata_20250116095815 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [DECIMAL(38,9), DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│ example  │ mydata_20250117034742 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                │ false     │\n",
       "│ example  │ mydata_20250117034742 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]      │ false     │\n",
       "│ example  │ mydata_20250117034742 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                        │ false     │\n",
       "│ example  │ mydata_20250117034742 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [BIGINT, DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR]        │ false     │\n",
       "│ example  │ mydata_20250117035246 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                │ false     │\n",
       "│ example  │ mydata_20250117035246 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]      │ false     │\n",
       "│ example  │ mydata_20250117035246 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                        │ false     │\n",
       "│ example  │ mydata_20250117035246 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [BIGINT, DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR]        │ false     │\n",
       "├──────────┴───────────────────────┴─────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────┴───────────┤\n",
       "│ 12 rows                                                                                                                                                                                                                                                    6 columns │\n",
       "└──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.sql('DESCRIBE;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬──────────────┐\n",
       "│  id   │ count_star() │\n",
       "│ int64 │    int64     │\n",
       "├───────┼──────────────┤\n",
       "│     1 │            1 │\n",
       "│     2 │            1 │\n",
       "│     3 │            1 │\n",
       "│     4 │            1 │\n",
       "│     5 │            1 │\n",
       "│     6 │            1 │\n",
       "│     7 │            1 │\n",
       "│     8 │            1 │\n",
       "│     9 │            1 │\n",
       "│    10 │            1 │\n",
       "│     · │            · │\n",
       "│     · │            · │\n",
       "│     · │            · │\n",
       "│    21 │            1 │\n",
       "│    22 │            1 │\n",
       "│    23 │            1 │\n",
       "│    24 │            1 │\n",
       "│    25 │            1 │\n",
       "│    26 │            1 │\n",
       "│    27 │            1 │\n",
       "│    28 │            1 │\n",
       "│    29 │            1 │\n",
       "│    30 │            1 │\n",
       "├───────┴──────────────┤\n",
       "│  30 rows (20 shown)  │\n",
       "└──────────────────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.sql('SELECT id, COUNT(*) FROM mydata_20250117034742.normal GROUP BY id;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use Pandas, the data can be accessed via [`ReadableDataset`](https://dlthub.com/docs/general-usage/dataset-access/dataset).\n",
    "\n",
    "In addition to transforming data using `duckdb` explicitly, you can use [the `dlt` SQL client](https://dlthub.com/docs/dlt-ecosystem/transformations/sql) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Marketing', 7),\n",
       " ('Customer Service', 5),\n",
       " ('IT', 3),\n",
       " ('HR', 4),\n",
       " ('Finance', 5),\n",
       " ('Sales', 6)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with pipeline_topic.sql_client() as p:\n",
    "    ans = p.execute_sql('SELECT category, COUNT(*) FROM mydata_20250117034742.normal GROUP BY category;')\n",
    "\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesystem_resource_task = filesystem(\n",
    "    bucket_url='file:data/joinable',\n",
    "    file_glob='*.csv'\n",
    ") \n",
    "\n",
    "filesystem_resource_task.add_filter(lambda item: item['file_name'] != 'j03.csv')\n",
    "\n",
    "filesystem_pipe_task = filesystem_resource_task | read_csv_duckdb()\n",
    "\n",
    "filesystem_pipe_task.apply_hints(write_disposition='replace', table_name='join')\n",
    "\n",
    "pipeline_task = dlt.pipeline(\n",
    "    pipeline_name='csv_load_join', \n",
    "    destination=dlt.destinations.duckdb('example.duckdb'), \n",
    "    dataset_name='mydata', dev_mode=True\n",
    ")\n",
    "\n",
    "load_info_task = pipeline_task.run(filesystem_pipe_task) # the hints can be passed here as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, Decimal('157.230000000'), 'Finance', 'john.doe@company.com', 'completed'), (2, Decimal('293.450000000'), 'Sales', 'alice.smith@company.com', 'completed'), (3, Decimal('432.180000000'), 'Customer Service', 'bob.wilson@company.com', 'in_progress'), (4, Decimal('567.890000000'), 'Marketing', 'sarah.jones@company.com', 'completed'), (5, Decimal('123.450000000'), 'HR', 'mike.brown@company.com', 'pending'), (6, Decimal('789.120000000'), 'Marketing', 'emma.davis@company.com', 'completed'), (7, Decimal('234.560000000'), 'Finance', 'james.miller@company.com', 'completed'), (8, Decimal('678.900000000'), 'Sales', 'olivia.wilson@company.com', 'in_progress'), (9, Decimal('345.670000000'), 'Customer Service', 'william.taylor@company.com', 'completed'), (10, Decimal('891.230000000'), 'Marketing', 'sophia.anderson@company.com', 'pending'), (11, Decimal('456.780000000'), 'HR', 'alexander.thomas@company.com', 'completed'), (12, Decimal('912.340000000'), 'Finance', 'isabella.martin@company.com', 'in_progress'), (13, Decimal('567.890000000'), 'Sales', 'ethan.white@company.com', 'completed'), (14, Decimal('234.560000000'), 'Customer Service', 'ava.johnson@company.com', 'pending'), (15, Decimal('789.010000000'), 'Marketing', 'noah.garcia@company.com', 'completed'), (16, Decimal('843.210000000'), 'IT', 'mia.martinez@company.com', 'in_progress'), (17, Decimal('654.320000000'), 'Sales', 'lucas.robinson@company.com', 'completed'), (18, Decimal('987.650000000'), 'Customer Service', 'emily.clark@company.com', 'completed'), (19, Decimal('321.980000000'), 'Marketing', 'daniel.rodriguez@company.com', 'pending'), (20, Decimal('765.430000000'), 'HR', 'victoria.lee@company.com', 'completed'), (21, Decimal('432.100000000'), 'Finance', 'david.walker@company.com', 'in_progress'), (22, Decimal('876.540000000'), 'Sales', 'sophia.hall@company.com', 'completed'), (23, Decimal('543.210000000'), 'IT', 'joseph.allen@company.com', 'pending'), (24, Decimal('198.760000000'), 'Customer Service', 'olivia.young@company.com', 'completed'), (25, Decimal('654.320000000'), 'Marketing', 'henry.king@company.com', 'in_progress'), (26, Decimal('987.650000000'), 'HR', 'elizabeth.wright@company.com', 'completed'), (27, Decimal('321.980000000'), 'Finance', 'alexander.lopez@company.com', 'pending'), (28, Decimal('765.430000000'), 'Sales', 'charlotte.hill@company.com', 'completed'), (29, Decimal('432.100000000'), 'IT', 'benjamin.scott@company.com', 'in_progress'), (30, Decimal('876.540000000'), 'Marketing', 'amelia.green@company.com', 'completed')]\n"
     ]
    }
   ],
   "source": [
    "with pipeline_task.sql_client() as p:\n",
    "    ans = p.execute_sql('SELECT n.id, value, category, assigned_to, status FROM mydata_20250117035246.normal AS n JOIN mydata_20250117035805.join AS j ON n.id=j.id;')\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy Preserving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudonymizing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
