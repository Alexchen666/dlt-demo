{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `dlt` Demo\n",
    "\n",
    "Resource: https://dlthub.com/docs/intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic\n",
    "\n",
    "`dlt` can be used in jupyter notebook or command line (config).\n",
    "\n",
    "You can create your own [transformer](https://dlthub.com/docs/dlt-ecosystem/verified-sources/filesystem/advanced#example-read-data-from-excel-files) to load excel files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "import duckdb\n",
    "\n",
    "# for data validation\n",
    "from pydantic import BaseModel, Field\n",
    "from datetime import datetime\n",
    "from typing import Literal, Optional\n",
    "from decimal import Decimal\n",
    "\n",
    "# using read_csv_duckdb is much more efficient than read_csv, which uses pandas\n",
    "from dlt.sources.filesystem import filesystem, read_csv_duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BusinessRecord(BaseModel):\n",
    "    \"\"\"Represents a single business record in the dataset.\"\"\"\n",
    "\n",
    "    id: int = Field(gt=0, description=\"Unique identifier for the record\")\n",
    "    value: Decimal = Field(decimal_places=2, description=\"Business metric value\")\n",
    "    timestamp: datetime = Field(description=\"Timestamp of the record\")\n",
    "    description: str = Field(\n",
    "        min_length=1, description=\"Description of the business activity\"\n",
    "    )\n",
    "    category: Literal[\n",
    "        \"Finance\", \"Sales\", \"Customer Service\", \"Marketing\", \"HR\", \"IT\"\n",
    "    ] = Field(description=\"Business department category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch data from the [file system](https://dlthub.com/docs/dlt-ecosystem/verified-sources/filesystem/basic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesystem_resource_topic = filesystem(bucket_url=\"file:data/normal\", file_glob=\"*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add filters (filter by name or size) at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dlt.extract.resource.DltResource at 0x103d19c40>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filesystem_resource_topic.add_filter(\n",
    "    lambda item: item[\"file_name\"]\n",
    "    not in [\"normal3.csv\", \"normal4_1.csv\", \"normal4_2.csv\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesystem_pipe_topic = filesystem_resource_topic | read_csv_duckdb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can apply hints (e.g., [incremental loading](https://dlthub.com/docs/general-usage/incremental-loading), i.e., only load the new data, create table name, and specify table schema) at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dlt.extract.resource.DltResource at 0x107756690>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filesystem_pipe_topic.apply_hints(\n",
    "    write_disposition=\"replace\", table_name=\"normal\", columns=BusinessRecord\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below generates a `example.duckdb` file. This file can be used in dbt via dbt-duckdb, see [this doc](https://dlthub.com/docs/dlt-ecosystem/destinations/duckdb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_topic = dlt.pipeline(\n",
    "    pipeline_name=\"csv_load\",\n",
    "    destination=dlt.destinations.duckdb(\"example.duckdb\"),\n",
    "    dataset_name=\"mydata\",\n",
    ")\n",
    "\n",
    "load_info = pipeline_topic.run(\n",
    "    filesystem_pipe_topic\n",
    ")  # the hints can be passed here as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pipeline_topic.default_schema.to_pretty_yaml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = duckdb.connect(database=\"example.duckdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌──────────┬───────────────────────┬─────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────┬─────────────────────────────────────────────────────────────────────────────────────────┬───────────┐\n",
       "│ database │        schema         │        name         │                                           column_names                                           │                                      column_types                                       │ temporary │\n",
       "│ varchar  │        varchar        │       varchar       │                                            varchar[]                                             │                                        varchar[]                                        │  boolean  │\n",
       "├──────────┼───────────────────────┼─────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────┼───────────┤\n",
       "│ example  │ mydata_20250120072503 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                           │ false     │\n",
       "│ example  │ mydata_20250120072503 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│ example  │ mydata_20250120072503 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                   │ false     │\n",
       "│ example  │ mydata_20250120072503 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [BIGINT, DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR]   │ false     │\n",
       "│ example  │ mydata_20250120072521 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                           │ false     │\n",
       "│ example  │ mydata_20250120072521 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│ example  │ mydata_20250120072521 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                   │ false     │\n",
       "│ example  │ mydata_20250120072521 │ join                │ [id, assigned_to, duration_minutes, status, cost_estimate, _dlt_load_id, _dlt_id]                │ [BIGINT, VARCHAR, BIGINT, VARCHAR, DOUBLE, VARCHAR, VARCHAR]                            │ false     │\n",
       "│ example  │ mydata_20250120084755 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                           │ false     │\n",
       "│ example  │ mydata_20250120084755 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│    ·     │           ·           │      ·              │                                     ·                                                            │                                   ·                                                     │   ·       │\n",
       "│    ·     │           ·           │      ·              │                                     ·                                                            │                                   ·                                                     │   ·       │\n",
       "│    ·     │           ·           │      ·              │                                     ·                                                            │                                   ·                                                     │   ·       │\n",
       "│ example  │ mydata_20250120090457 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                   │ false     │\n",
       "│ example  │ mydata_20250120090457 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [BIGINT, DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR]   │ false     │\n",
       "│ example  │ mydata_20250120090458 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                           │ false     │\n",
       "│ example  │ mydata_20250120090458 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│ example  │ mydata_20250120090458 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                   │ false     │\n",
       "│ example  │ mydata_20250120090458 │ join                │ [id, assigned_to, duration_minutes, status, cost_estimate, _dlt_load_id, _dlt_id]                │ [BIGINT, VARCHAR, BIGINT, VARCHAR, DOUBLE, VARCHAR, VARCHAR]                            │ false     │\n",
       "│ example  │ mydata_20250120090544 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                           │ false     │\n",
       "│ example  │ mydata_20250120090544 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│ example  │ mydata_20250120090544 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                   │ false     │\n",
       "│ example  │ mydata_20250120090544 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [BIGINT, DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR]   │ false     │\n",
       "├──────────┴───────────────────────┴─────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────┴─────────────────────────────────────────────────────────────────────────────────────────┴───────────┤\n",
       "│ 69 rows (20 shown)                                                                                                                                                                                                                                    6 columns │\n",
       "└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.sql(\"DESCRIBE;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.sql(\"SELECT id, COUNT(*) FROM mydata.normal GROUP BY id;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use Pandas, the data can be accessed via [`ReadableDataset`](https://dlthub.com/docs/general-usage/dataset-access/dataset).\n",
    "\n",
    "In addition to transforming data using `duckdb` explicitly, you can use [the `dlt` SQL client](https://dlthub.com/docs/dlt-ecosystem/transformations/sql) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pipeline_topic.sql_client() as p:\n",
    "#     ans = p.execute_sql(\n",
    "#         \"SELECT category, COUNT(*) FROM mydata.normal GROUP BY category;\"\n",
    "#     )\n",
    "\n",
    "# ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesystem_resource_task = filesystem(\n",
    "    bucket_url=\"file:data/joinable\", file_glob=\"*.csv\"\n",
    ")\n",
    "\n",
    "filesystem_resource_task.add_filter(lambda item: item[\"file_name\"] != \"j03.csv\")\n",
    "\n",
    "filesystem_pipe_task = filesystem_resource_task | read_csv_duckdb()\n",
    "\n",
    "filesystem_pipe_task.apply_hints(write_disposition=\"replace\", table_name=\"join\")\n",
    "\n",
    "pipeline_task = dlt.pipeline(\n",
    "    pipeline_name=\"csv_load_join\",\n",
    "    destination=dlt.destinations.duckdb(\"example.duckdb\"),\n",
    "    dataset_name=\"mydata\",\n",
    "    dev_mode=True,\n",
    ")\n",
    "\n",
    "load_info_task = pipeline_task.run(\n",
    "    filesystem_pipe_task\n",
    ")  # the hints can be passed here as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pipeline_task.sql_client() as p:\n",
    "#     ans = p.execute_sql(\n",
    "#         \"SELECT n.id, value, category, assigned_to, status FROM mydata.normal AS n JOIN mydata.join AS j ON n.id=j.id;\"\n",
    "#     )\n",
    "\n",
    "# print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to creating a Pydantic schema, you can [adjust a schema](https://dlthub.com/docs/walkthroughs/adjust-a-schema) based on the autogenerated one.\n",
    "\n",
    "To be specific, [schema](https://dlthub.com/docs/general-usage/schema) \"describes the structure of normalized data (e.g., tables, columns, data types, etc.) and provides instructions on how the data should be processed and loaded.\" And normalized means that `dlt` will change the structure of the input data, such as feature names, data types, etc., to load it into the destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_topic_schema = dlt.pipeline(\n",
    "    import_schema_path=\"schemas/import\",  # path to the schema file (imported schema)\n",
    "    export_schema_path=\"schemas/export\",  # path to the schema file (exported schema)\n",
    "    pipeline_name=\"csv_load_schema\",\n",
    "    destination=dlt.destinations.duckdb(\"example_schema.duckdb\"),\n",
    "    dataset_name=\"mydata_schema\",\n",
    "    dev_mode=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadInfo(pipeline=<dlt.pipeline.pipeline.Pipeline object at 0x110020d40>, metrics={'1737363945.662146': [{'started_at': DateTime(2025, 1, 20, 9, 5, 45, 776708, tzinfo=Timezone('UTC')), 'finished_at': DateTime(2025, 1, 20, 9, 5, 45, 865676, tzinfo=Timezone('UTC')), 'job_metrics': {'_dlt_pipeline_state.287409741f.insert_values': LoadJobMetrics(job_id='_dlt_pipeline_state.287409741f.insert_values', file_path='/Users/alex/.dlt/pipelines/csv_load_schema/load/normalized/1737363945.662146/started_jobs/_dlt_pipeline_state.287409741f.0.insert_values', table_name='_dlt_pipeline_state', started_at=DateTime(2025, 1, 20, 9, 5, 45, 834119, tzinfo=Timezone('UTC')), finished_at=DateTime(2025, 1, 20, 9, 5, 45, 835030, tzinfo=Timezone('UTC')), state='completed', remote_url=None), 'normal.688ee9342f.insert_values': LoadJobMetrics(job_id='normal.688ee9342f.insert_values', file_path='/Users/alex/.dlt/pipelines/csv_load_schema/load/normalized/1737363945.662146/started_jobs/normal.688ee9342f.0.insert_values', table_name='normal', started_at=DateTime(2025, 1, 20, 9, 5, 45, 834054, tzinfo=Timezone('UTC')), finished_at=DateTime(2025, 1, 20, 9, 5, 45, 835554, tzinfo=Timezone('UTC')), state='completed', remote_url=None)}}]}, destination_type='dlt.destinations.duckdb', destination_displayable_credentials='duckdb:////Users/alex/CAPE/dlt-demo/example_schema.duckdb', destination_name='duckdb', environment=None, staging_type=None, staging_name=None, staging_displayable_credentials=None, destination_fingerprint='', dataset_name='mydata_schema_20250120090545', loads_ids=['1737363945.662146'], load_packages=[LoadPackageInfo(load_id='1737363945.662146', package_path='/Users/alex/.dlt/pipelines/csv_load_schema/load/loaded/1737363945.662146', state='loaded', schema=Schema csv_load_schema at 4563817136, schema_update={'normal': {'columns': {'id': {'data_type': 'decimal', 'nullable': False, 'name': 'id'}, 'value': {'data_type': 'decimal', 'nullable': False, 'name': 'value'}, 'timestamp': {'data_type': 'timestamp', 'nullable': False, 'name': 'timestamp'}, 'description': {'data_type': 'text', 'nullable': False, 'name': 'description'}, 'category': {'data_type': 'text', 'nullable': False, 'name': 'category'}, '_dlt_load_id': {'name': '_dlt_load_id', 'data_type': 'text', 'nullable': False}, '_dlt_id': {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True, 'row_key': True}}, 'write_disposition': 'replace', 'schema_contract': {'tables': 'evolve', 'columns': 'discard_value', 'data_type': 'freeze'}, 'resource': '_read_csv_duckdb', 'name': 'normal', 'x-normalizer': {'seen-data': True}}, '_dlt_pipeline_state': {'columns': {'version': {'data_type': 'bigint', 'nullable': False, 'name': 'version'}, 'engine_version': {'data_type': 'bigint', 'nullable': False, 'name': 'engine_version'}, 'pipeline_name': {'data_type': 'text', 'nullable': False, 'name': 'pipeline_name'}, 'state': {'data_type': 'text', 'nullable': False, 'name': 'state'}, 'created_at': {'data_type': 'timestamp', 'nullable': False, 'name': 'created_at'}, 'version_hash': {'data_type': 'text', 'nullable': True, 'name': 'version_hash'}, '_dlt_load_id': {'data_type': 'text', 'nullable': False, 'name': '_dlt_load_id'}, '_dlt_id': {'name': '_dlt_id', 'data_type': 'text', 'nullable': False, 'unique': True, 'row_key': True}}, 'write_disposition': 'append', 'file_format': 'preferred', 'resource': '_dlt_pipeline_state', 'name': '_dlt_pipeline_state', 'x-normalizer': {'seen-data': True}}, '_dlt_version': {'columns': {'version': {'data_type': 'bigint', 'nullable': False, 'name': 'version'}, 'engine_version': {'data_type': 'bigint', 'nullable': False, 'name': 'engine_version'}, 'inserted_at': {'data_type': 'timestamp', 'nullable': False, 'name': 'inserted_at'}, 'schema_name': {'data_type': 'text', 'nullable': False, 'name': 'schema_name'}, 'version_hash': {'data_type': 'text', 'nullable': False, 'name': 'version_hash'}, 'schema': {'data_type': 'text', 'nullable': False, 'name': 'schema'}}, 'write_disposition': 'skip', 'resource': '_dlt_version', 'description': 'Created by DLT. Tracks schema updates', 'name': '_dlt_version'}, '_dlt_loads': {'columns': {'load_id': {'data_type': 'text', 'nullable': False, 'name': 'load_id'}, 'schema_name': {'data_type': 'text', 'nullable': True, 'name': 'schema_name'}, 'status': {'data_type': 'bigint', 'nullable': False, 'name': 'status'}, 'inserted_at': {'data_type': 'timestamp', 'nullable': False, 'name': 'inserted_at'}, 'schema_version_hash': {'data_type': 'text', 'nullable': True, 'name': 'schema_version_hash'}}, 'write_disposition': 'skip', 'resource': '_dlt_loads', 'description': 'Created by DLT. Tracks completed loads', 'name': '_dlt_loads'}}, completed_at=DateTime(2025, 1, 20, 9, 5, 45, 863294, tzinfo=Timezone('UTC')), jobs={'new_jobs': [], 'failed_jobs': [], 'started_jobs': [], 'completed_jobs': [LoadJobInfo(state='completed_jobs', file_path='/Users/alex/.dlt/pipelines/csv_load_schema/load/loaded/1737363945.662146/completed_jobs/normal.688ee9342f.0.insert_values', file_size=1401, created_at=DateTime(2025, 1, 20, 9, 5, 45, 752654, tzinfo=Timezone('UTC')), elapsed=0.11063981056213379, job_file_info=ParsedLoadJobFileName(table_name='normal', file_id='688ee9342f', retry_count=0, file_format='insert_values'), failed_message=None), LoadJobInfo(state='completed_jobs', file_path='/Users/alex/.dlt/pipelines/csv_load_schema/load/loaded/1737363945.662146/completed_jobs/_dlt_pipeline_state.287409741f.0.insert_values', file_size=528, created_at=DateTime(2025, 1, 20, 9, 5, 45, 752395, tzinfo=Timezone('UTC')), elapsed=0.11089873313903809, job_file_info=ParsedLoadJobFileName(table_name='_dlt_pipeline_state', file_id='287409741f', retry_count=0, file_format='insert_values'), failed_message=None)]})], first_run=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_topic_schema.run(filesystem_pipe_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌────────────────┬──────────────────────────────┬─────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────┬───────────┐\n",
       "│    database    │            schema            │        name         │                                           column_names                                           │                                         column_types                                         │ temporary │\n",
       "│    varchar     │           varchar            │       varchar       │                                            varchar[]                                             │                                          varchar[]                                           │  boolean  │\n",
       "├────────────────┼──────────────────────────────┼─────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────┼───────────┤\n",
       "│ example_schema │ mydata_schema_20250120072540 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                │ false     │\n",
       "│ example_schema │ mydata_schema_20250120072540 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]      │ false     │\n",
       "│ example_schema │ mydata_schema_20250120072540 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                        │ false     │\n",
       "│ example_schema │ mydata_schema_20250120072540 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [DECIMAL(38,9), DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│ example_schema │ mydata_schema_20250120084827 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                │ false     │\n",
       "│ example_schema │ mydata_schema_20250120084827 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]      │ false     │\n",
       "│ example_schema │ mydata_schema_20250120084827 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                        │ false     │\n",
       "│ example_schema │ mydata_schema_20250120084827 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [DECIMAL(38,9), DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│ example_schema │ mydata_schema_20250120084915 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                │ false     │\n",
       "│ example_schema │ mydata_schema_20250120084915 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]      │ false     │\n",
       "│       ·        │              ·               │      ·              │                                     ·                                                            │                                   ·                                                          │   ·       │\n",
       "│       ·        │              ·               │      ·              │                                     ·                                                            │                                   ·                                                          │   ·       │\n",
       "│       ·        │              ·               │      ·              │                                     ·                                                            │                                   ·                                                          │   ·       │\n",
       "│ example_schema │ mydata_schema_20250120090120 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                        │ false     │\n",
       "│ example_schema │ mydata_schema_20250120090120 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [DECIMAL(38,9), DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│ example_schema │ mydata_schema_20250120090458 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                │ false     │\n",
       "│ example_schema │ mydata_schema_20250120090458 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]      │ false     │\n",
       "│ example_schema │ mydata_schema_20250120090458 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                        │ false     │\n",
       "│ example_schema │ mydata_schema_20250120090458 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [DECIMAL(38,9), DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│ example_schema │ mydata_schema_20250120090545 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                │ false     │\n",
       "│ example_schema │ mydata_schema_20250120090545 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]      │ false     │\n",
       "│ example_schema │ mydata_schema_20250120090545 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                        │ false     │\n",
       "│ example_schema │ mydata_schema_20250120090545 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [DECIMAL(38,9), DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "├────────────────┴──────────────────────────────┴─────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────┴───────────┤\n",
       "│ 44 rows (20 shown)                                                                                                                                                                                                                                                      6 columns │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = duckdb.connect(database=\"example_schema.duckdb\")\n",
    "db.sql(\"DESCRIBE;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the folders and schemas are created, you can edit the schema in the `import` folder to take effect.\n",
    "\n",
    "Acceptable data types: `['text', 'double', 'bool', 'timestamp', 'bigint', 'binary', 'json', 'decimal', 'wei', 'date', 'time']`\n",
    "\n",
    "Note: You should keep the import schema as simple as possible and let dlt do the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadInfo(pipeline=<dlt.pipeline.pipeline.Pipeline object at 0x110020d40>, metrics={'1737363945.95684': [{'started_at': DateTime(2025, 1, 20, 9, 5, 46, 29876, tzinfo=Timezone('UTC')), 'finished_at': DateTime(2025, 1, 20, 9, 5, 46, 82863, tzinfo=Timezone('UTC')), 'job_metrics': {'normal.6e2610c9c1.insert_values': LoadJobMetrics(job_id='normal.6e2610c9c1.insert_values', file_path='/Users/alex/.dlt/pipelines/csv_load_schema/load/normalized/1737363945.95684/started_jobs/normal.6e2610c9c1.0.insert_values', table_name='normal', started_at=DateTime(2025, 1, 20, 9, 5, 46, 51939, tzinfo=Timezone('UTC')), finished_at=DateTime(2025, 1, 20, 9, 5, 46, 53279, tzinfo=Timezone('UTC')), state='completed', remote_url=None)}}]}, destination_type='dlt.destinations.duckdb', destination_displayable_credentials='duckdb:////Users/alex/CAPE/dlt-demo/example_schema.duckdb', destination_name='duckdb', environment=None, staging_type=None, staging_name=None, staging_displayable_credentials=None, destination_fingerprint='', dataset_name='mydata_schema_20250120090545', loads_ids=['1737363945.95684'], load_packages=[LoadPackageInfo(load_id='1737363945.95684', package_path='/Users/alex/.dlt/pipelines/csv_load_schema/load/loaded/1737363945.95684', state='loaded', schema=Schema csv_load_schema at 4564019792, schema_update={}, completed_at=DateTime(2025, 1, 20, 9, 5, 46, 80393, tzinfo=Timezone('UTC')), jobs={'new_jobs': [], 'failed_jobs': [], 'started_jobs': [], 'completed_jobs': [LoadJobInfo(state='completed_jobs', file_path='/Users/alex/.dlt/pipelines/csv_load_schema/load/loaded/1737363945.95684/completed_jobs/normal.6e2610c9c1.0.insert_values', file_size=1407, created_at=DateTime(2025, 1, 20, 9, 5, 46, 12476, tzinfo=Timezone('UTC')), elapsed=0.0679173469543457, job_file_info=ParsedLoadJobFileName(table_name='normal', file_id='6e2610c9c1', retry_count=0, file_format='insert_values'), failed_message=None)]})], first_run=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_topic_schema.run(filesystem_pipe_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌────────────────┬──────────────────────────────┬─────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────┬───────────┐\n",
       "│    database    │            schema            │        name         │                                           column_names                                           │                                         column_types                                         │ temporary │\n",
       "│    varchar     │           varchar            │       varchar       │                                            varchar[]                                             │                                          varchar[]                                           │  boolean  │\n",
       "├────────────────┼──────────────────────────────┼─────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────┼───────────┤\n",
       "│ example_schema │ mydata_schema_20250120072540 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                │ false     │\n",
       "│ example_schema │ mydata_schema_20250120072540 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]      │ false     │\n",
       "│ example_schema │ mydata_schema_20250120072540 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                        │ false     │\n",
       "│ example_schema │ mydata_schema_20250120072540 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [DECIMAL(38,9), DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│ example_schema │ mydata_schema_20250120084827 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                │ false     │\n",
       "│ example_schema │ mydata_schema_20250120084827 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]      │ false     │\n",
       "│ example_schema │ mydata_schema_20250120084827 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                        │ false     │\n",
       "│ example_schema │ mydata_schema_20250120084827 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [DECIMAL(38,9), DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│ example_schema │ mydata_schema_20250120084915 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                │ false     │\n",
       "│ example_schema │ mydata_schema_20250120084915 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]      │ false     │\n",
       "│       ·        │              ·               │      ·              │                                     ·                                                            │                                   ·                                                          │   ·       │\n",
       "│       ·        │              ·               │      ·              │                                     ·                                                            │                                   ·                                                          │   ·       │\n",
       "│       ·        │              ·               │      ·              │                                     ·                                                            │                                   ·                                                          │   ·       │\n",
       "│ example_schema │ mydata_schema_20250120090120 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                        │ false     │\n",
       "│ example_schema │ mydata_schema_20250120090120 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [DECIMAL(38,9), DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│ example_schema │ mydata_schema_20250120090458 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                │ false     │\n",
       "│ example_schema │ mydata_schema_20250120090458 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]      │ false     │\n",
       "│ example_schema │ mydata_schema_20250120090458 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                        │ false     │\n",
       "│ example_schema │ mydata_schema_20250120090458 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [DECIMAL(38,9), DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│ example_schema │ mydata_schema_20250120090545 │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                                │ false     │\n",
       "│ example_schema │ mydata_schema_20250120090545 │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]      │ false     │\n",
       "│ example_schema │ mydata_schema_20250120090545 │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                        │ false     │\n",
       "│ example_schema │ mydata_schema_20250120090545 │ normal              │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [DECIMAL(38,9), DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "├────────────────┴──────────────────────────────┴─────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────┴───────────┤\n",
       "│ 44 rows (20 shown)                                                                                                                                                                                                                                                      6 columns │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = duckdb.connect(database=\"example_schema.duckdb\")\n",
    "db.sql(\"DESCRIBE;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contract\n",
    "\n",
    "[Contracts](https://dlthub.com/docs/general-usage/schema-contracts) define how a schema evolve with the future data. There are three levels: `tables`, `columns`, `data_type`, and four actions: `evolve`, `freeze`, `discard_row`, `discard_value`.\n",
    "\n",
    "|  | `evolve` | `freeze` | `discard_row` | `discard_value` |\n",
    "|---|---|---|---|---|\n",
    "| `tables` | Allow to add new tables | Error, not allow to add new tables | Only add metadata, all data is discarded | Only add metadata, all data is discarded |\n",
    "| `columns` | Allow to add new columns | Error, not allow to add new columns | The rows with new column(s) are discarded | The new column(s) are discarded |\n",
    "| `data_type` | Use [variant column](https://dlthub.com/docs/general-usage/schema#variant-columns) | Error, not allow to use variant column | The rows with different column(s) are discarded | The value with different column(s) are discarded |\n",
    "\n",
    "Note: Under `tables` scope, the table with same name is still acceptable. The constraint is only on the table with a different name.\n",
    "\n",
    "Note: For coercible data type, `dlt` coerces the data type implicitly regardless of contracts. The table above shows how `dlt` deals with non-coercible data types with contracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-21 15:15:57,691|[WARNING]|8164|8247591488|dlt|configuration.py|_path_from_pipeline:178|Duckdb attached to pipeline csv_load_join_con in path example_con.duckdb was could not be found but pipeline has already ran. This may be a result of (1) recreating or attaching pipeline  without or with changed explicit path to database that was used when creating the pipeline. (2) keeping the path to to database in secrets and changing the current working folder so  dlt cannot see them. (3) you deleting the database.\n"
     ]
    }
   ],
   "source": [
    "# demo for table and column contract\n",
    "# filesystem_resource_task_con = filesystem(\n",
    "#     bucket_url=\"file:data/normal\", file_glob=\"*.csv\"\n",
    "# )\n",
    "\n",
    "# filesystem_resource_task_con.add_filter(lambda item: item[\"file_name\"] not in [\"normal3.csv\", \"normal4_1.csv\", \"normal4_2.csv\"])\n",
    "\n",
    "# filesystem_pipe_task_con = filesystem_resource_task_con | read_csv_duckdb()\n",
    "\n",
    "# filesystem_pipe_task_con.apply_hints(write_disposition=\"append\", table_name=\"join\")\n",
    "\n",
    "# pipeline_task_con = dlt.pipeline(\n",
    "#     pipeline_name=\"csv_load_join_con\",\n",
    "#     destination=dlt.destinations.duckdb(\"example_con.duckdb\"),\n",
    "#     dataset_name=\"mydata\",\n",
    "# )\n",
    "\n",
    "# load_info_task_con = pipeline_task_con.run(\n",
    "#     filesystem_pipe_task_con,\n",
    "#     schema_contract={\"tables\": \"evolve\", \"columns\": \"evolve\"},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo for table contract\n",
    "# filesystem_resource_task_con2 = filesystem(\n",
    "#     bucket_url=\"file:data/normal\", file_glob=\"normal3.csv\"\n",
    "# )\n",
    "\n",
    "# filesystem_pipe_task_con2 = filesystem_resource_task_con2 | read_csv_duckdb()\n",
    "\n",
    "# filesystem_pipe_task_con2.apply_hints(write_disposition=\"append\", table_name=\"join\")\n",
    "\n",
    "# load_info_task_con2 = pipeline_task_con.run(\n",
    "#     filesystem_pipe_task_con2, schema_contract={\"tables\": \"freeze\"}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo for column contract\n",
    "# filesystem_resource_task_con2 = filesystem(\n",
    "#     bucket_url=\"file:data/normal\", file_glob=\"normal3.csv\"\n",
    "# )\n",
    "\n",
    "# filesystem_pipe_task_con2 = filesystem_resource_task_con2 | read_csv_duckdb()\n",
    "\n",
    "# filesystem_pipe_task_con2.apply_hints(write_disposition=\"append\", table_name=\"join\")\n",
    "\n",
    "# load_info_task_con2 = pipeline_task_con.run(\n",
    "#     filesystem_pipe_task_con2, schema_contract={\"tables\": \"evolve\", \"columns\": \"discard_value\"}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-21 15:38:29,798|[WARNING]|8164|8247591488|dlt|configuration.py|_path_from_pipeline:178|Duckdb attached to pipeline csv_load_join_con in path example_con.duckdb was could not be found but pipeline has already ran. This may be a result of (1) recreating or attaching pipeline  without or with changed explicit path to database that was used when creating the pipeline. (2) keeping the path to to database in secrets and changing the current working folder so  dlt cannot see them. (3) you deleting the database.\n"
     ]
    }
   ],
   "source": [
    "# demo for data type contract\n",
    "# class BusinessRecord2(BaseModel):\n",
    "#     \"\"\"Represents a single business record in the dataset.\"\"\"\n",
    "\n",
    "#     id: Optional[int] = Field(\n",
    "#         gt=0, description=\"Unique identifier for the record\"\n",
    "#     )  # to focus on data types, use Optional to allow NULL\n",
    "#     value: Decimal = Field(decimal_places=2, description=\"Business metric value\")\n",
    "#     timestamp: datetime = Field(description=\"Timestamp of the record\")\n",
    "#     description: str = Field(\n",
    "#         min_length=1, description=\"Description of the business activity\"\n",
    "#     )\n",
    "#     category: Literal[\n",
    "#         \"Finance\", \"Sales\", \"Customer Service\", \"Marketing\", \"HR\", \"IT\"\n",
    "#     ] = Field(description=\"Business department category\")\n",
    "\n",
    "\n",
    "# filesystem_resource_task_con = filesystem(\n",
    "#     bucket_url=\"file:data/normal\", file_glob=\"*.csv\"\n",
    "# )\n",
    "\n",
    "# filesystem_resource_task_con.add_filter(\n",
    "#     lambda item: item[\"file_name\"]\n",
    "#     not in [\"normal3.csv\", \"normal4_1.csv\", \"normal4_2.csv\"]\n",
    "# )\n",
    "\n",
    "# filesystem_pipe_task_con = filesystem_resource_task_con | read_csv_duckdb()\n",
    "\n",
    "# filesystem_pipe_task_con.apply_hints(\n",
    "#     write_disposition=\"append\", table_name=\"join\", columns=BusinessRecord2\n",
    "# )\n",
    "\n",
    "# pipeline_task_con = dlt.pipeline(\n",
    "#     pipeline_name=\"csv_load_join_con\",\n",
    "#     destination=dlt.destinations.duckdb(\"example_con.duckdb\"),\n",
    "#     dataset_name=\"mydata\",\n",
    "# )\n",
    "\n",
    "# load_info_task_con = pipeline_task_con.run(\n",
    "#     filesystem_pipe_task_con,\n",
    "#     schema_contract={\"tables\": \"evolve\", \"columns\": \"evolve\", \"data_type\": \"evolve\"},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filesystem_resource_task_con2 = filesystem(\n",
    "#     bucket_url=\"file:data/normal\", file_glob=\"normal4_2.csv\"\n",
    "# )\n",
    "\n",
    "# filesystem_pipe_task_con2 = filesystem_resource_task_con2 | read_csv_duckdb()\n",
    "\n",
    "# filesystem_pipe_task_con2.apply_hints(write_disposition=\"append\", table_name=\"join\")\n",
    "\n",
    "# load_info_task_con2 = pipeline_task_con.run(\n",
    "#     filesystem_pipe_task_con2,\n",
    "#     schema_contract={\n",
    "#         \"tables\": \"evolve\",\n",
    "#         \"columns\": \"evolve\",\n",
    "#         \"data_type\": \"discard_value\",\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────────┬─────────┬─────────────────────┬──────────────────────────────────────────────────────────────────────────────────────────────────┬─────────────────────────────────────────────────────────────────────────────────────────┬───────────┐\n",
       "│  database   │ schema  │        name         │                                           column_names                                           │                                      column_types                                       │ temporary │\n",
       "│   varchar   │ varchar │       varchar       │                                            varchar[]                                             │                                        varchar[]                                        │  boolean  │\n",
       "├─────────────┼─────────┼─────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────┼───────────┤\n",
       "│ example_con │ mydata  │ _dlt_loads          │ [load_id, schema_name, status, inserted_at, schema_version_hash]                                 │ [VARCHAR, VARCHAR, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR]                           │ false     │\n",
       "│ example_con │ mydata  │ _dlt_pipeline_state │ [version, engine_version, pipeline_name, state, created_at, version_hash, _dlt_load_id, _dlt_id] │ [BIGINT, BIGINT, VARCHAR, VARCHAR, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR] │ false     │\n",
       "│ example_con │ mydata  │ _dlt_version        │ [version, engine_version, inserted_at, schema_name, version_hash, schema]                        │ [BIGINT, BIGINT, TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR]                   │ false     │\n",
       "│ example_con │ mydata  │ join                │ [id, value, timestamp, description, category, _dlt_load_id, _dlt_id]                             │ [BIGINT, DECIMAL(38,9), TIMESTAMP WITH TIME ZONE, VARCHAR, VARCHAR, VARCHAR, VARCHAR]   │ false     │\n",
       "└─────────────┴─────────┴─────────────────────┴──────────────────────────────────────────────────────────────────────────────────────────────────┴─────────────────────────────────────────────────────────────────────────────────────────┴───────────┘"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = duckdb.connect(database=\"example_con.duckdb\")\n",
    "db.sql(\"DESCRIBE;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬───────────────┬──────────────────────────┬────────────────────────────────┬──────────────────┬───────────────────┬────────────────┐\n",
       "│  id   │     value     │        timestamp         │          description           │     category     │   _dlt_load_id    │    _dlt_id     │\n",
       "│ int64 │ decimal(38,9) │ timestamp with time zone │            varchar             │     varchar      │      varchar      │    varchar     │\n",
       "├───────┼───────────────┼──────────────────────────┼────────────────────────────────┼──────────────────┼───────────────────┼────────────────┤\n",
       "│     1 │ 157.230000000 │ 2024-01-01 17:00:00+08   │ Annual financial report review │ Finance          │ 1737445109.842436 │ UbH7MEYu9WMJlw │\n",
       "│     2 │ 293.450000000 │ 2024-01-01 18:30:00+08   │ Quarterly sales analysis       │ Sales            │ 1737445109.842436 │ x8IDMupsTc/A8Q │\n",
       "│     3 │ 432.180000000 │ 2024-01-01 19:45:00+08   │ Customer feedback summary      │ Customer Service │ 1737445109.842436 │ yv7SB8JAjqJAhw │\n",
       "│     4 │ 567.890000000 │ 2024-01-01 21:15:00+08   │ Product launch presentation    │ Marketing        │ 1737445109.842436 │ GCXf3oKQbUPj/w │\n",
       "│     5 │ 123.450000000 │ 2024-01-01 22:30:00+08   │ Team performance evaluation    │ HR               │ 1737445109.842436 │ 3sPUIzzRMIeazA │\n",
       "│     6 │ 789.120000000 │ 2024-01-01 23:45:00+08   │ Market research findings       │ Marketing        │ 1737445109.842436 │ BAAohAdHV9J+nA │\n",
       "│     7 │ 234.560000000 │ 2024-01-02 00:00:00+08   │ Budget planning meeting        │ Finance          │ 1737445109.842436 │ VEpfCx4eTQ2Orw │\n",
       "│     8 │ 678.900000000 │ 2024-01-02 17:30:00+08   │ Sales team training            │ Sales            │ 1737445109.842436 │ yCTwryW1Pquy4Q │\n",
       "│     9 │ 345.670000000 │ 2024-01-02 18:45:00+08   │ Customer survey results        │ Customer Service │ 1737445109.842436 │ Pzuarwb7G18Kcg │\n",
       "│    10 │ 891.230000000 │ 2024-01-02 19:30:00+08   │ Social media campaign          │ Marketing        │ 1737445109.842436 │ l7epE+0kxaS9ww │\n",
       "│     · │       ·       │           ·              │           ·                    │ ·                │         ·         │       ·        │\n",
       "│     · │       ·       │           ·              │           ·                    │ ·                │         ·         │       ·        │\n",
       "│     · │       ·       │           ·              │           ·                    │ ·                │         ·         │       ·        │\n",
       "│    23 │ 543.210000000 │ 2024-01-04 17:15:00+08   │ IT infrastructure upgrade      │ IT               │ 1737445109.842436 │ UJAC2I0iDohKLQ │\n",
       "│    24 │ 198.760000000 │ 2024-01-04 18:30:00+08   │ Customer experience workshop   │ Customer Service │ 1737445109.842436 │ S9Ts9s4TwRgeGw │\n",
       "│    25 │ 654.320000000 │ 2024-01-04 19:45:00+08   │ Content marketing plan         │ Marketing        │ 1737445109.842436 │ J/y5cyfSiy1zUA │\n",
       "│    26 │ 987.650000000 │ 2024-01-04 21:00:00+08   │ Recruitment campaign results   │ HR               │ 1737445109.842436 │ ZKG3FhXdMKn+7A │\n",
       "│    27 │ 321.980000000 │ 2024-01-04 22:15:00+08   │ Tax compliance review          │ Finance          │ 1737445109.842436 │ cI9Q/zMjlsl6jw │\n",
       "│    28 │ 765.430000000 │ 2024-01-04 23:30:00+08   │ Sales territory mapping        │ Sales            │ 1737445109.842436 │ ABLurHSCz12fyw │\n",
       "│    29 │ 432.100000000 │ 2024-01-05 00:45:00+08   │ Help desk performance metrics  │ IT               │ 1737445109.842436 │ 5PWdUFaqorFmiQ │\n",
       "│    30 │ 876.540000000 │ 2024-01-05 01:00:00+08   │ Brand partnership proposal     │ Marketing        │ 1737445109.842436 │ 0f6foauJijISRQ │\n",
       "│  NULL │ 857.440000000 │ 2024-01-06 21:00:00+08   │ Game marketing campaign        │ Marketing        │ 1737445110.634702 │ wR0bg821E2uhTg │\n",
       "│  NULL │ 943.110000000 │ 2024-01-06 22:45:00+08   │ Leadership training program    │ HR               │ 1737445110.634702 │ 6gPh9tyOVbbkNw │\n",
       "├───────┴───────────────┴──────────────────────────┴────────────────────────────────┴──────────────────┴───────────────────┴────────────────┤\n",
       "│ 32 rows (20 shown)                                                                                                              7 columns │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.sql(\"SELECT * FROM mydata.join;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
